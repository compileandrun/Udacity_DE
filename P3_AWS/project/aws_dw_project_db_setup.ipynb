{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4559177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import sql_queries\n",
    "import psycopg2\n",
    "import create_tables\n",
    "import etl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5707e",
   "metadata": {},
   "source": [
    "### I read the config file again to access and interact with the Redshift Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671aa095",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('project_dwh.cfg'))\n",
    "\n",
    "DWH_IAM_ROLE_NAME = config.get('IAM_ROLE','DWH_IAM_ROLE_NAME')\n",
    "\n",
    "DB_NAME = config.get('CLUSTER','DB_NAME')\n",
    "HOST = config.get('CLUSTER','HOST')\n",
    "DB_USER = config.get('CLUSTER','DB_USER')\n",
    "DB_PASSWORD = config.get('CLUSTER','DB_PASSWORD')\n",
    "DB_PORT = config.get('CLUSTER','DB_PORT')\n",
    "DWH_ENDPOINT = config.get('CLUSTER','DWH_ENDPOINT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b1bce",
   "metadata": {},
   "source": [
    "### I initiated the connection to the Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc5c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(DWH_ENDPOINT,DB_NAME,DB_USER,DB_PASSWORD,DB_PORT))\n",
    "cur = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d53711",
   "metadata": {},
   "source": [
    "### Here I drop and recreate the necessary tables: the 2 staging tables and other star schema tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c93d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS staging_events CASCADE table dropped\n",
      "dropped\n",
      "DROP TABLE IF EXISTS staging_songs CASCADE table dropped\n",
      "dropped\n",
      "DROP TABLE IF EXISTS songplay CASCADE table dropped\n",
      "dropped\n",
      "DROP TABLE IF EXISTS users CASCADE table dropped\n",
      "dropped\n",
      "DROP TABLE IF EXISTS songs CASCADE table dropped\n",
      "dropped\n",
      "DROP TABLE IF EXISTS artists CASCADE table dropped\n",
      "dropped\n",
      "DROP TABLE IF EXISTS playtimes CASCADE table dropped\n",
      "dropped\n",
      "table created\n",
      "table created\n",
      "table created\n",
      "table created\n",
      "table created\n",
      "table created\n",
      "table created\n"
     ]
    }
   ],
   "source": [
    "create_tables.drop_tables(cur,conn)\n",
    "create_tables.create_tables(cur,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9950467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2e93f",
   "metadata": {},
   "source": [
    "### Here I load the 2 staging tables with the data from S3 buckets.\n",
    "#### To save time, I only used the files in the A/A/ folder of the song_data bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1edd5ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    copy staging_events from 's3://udacity-dend/log_data/2018/11'\n",
      "    credentials 'aws_iam_role=arn:aws:iam::672895613677:role/dwhRole'\n",
      "    format json as 's3://udacity-dend/log_json_path.json'     \n",
      "    dateformat 'auto';\n",
      "\n",
      "\n",
      "    copy staging_songs from 's3://udacity-dend/song_data/A/A'\n",
      "    credentials 'aws_iam_role=arn:aws:iam::672895613677:role/dwhRole'\n",
      "    format as json 'auto' \n",
      "    region 'us-west-2';\n",
      "\n"
     ]
    }
   ],
   "source": [
    "etl.load_staging_tables(cur,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c996d20",
   "metadata": {},
   "source": [
    "### The data in the staging tables are loaded to the production tables inside the Redshift Cluster.\n",
    "#### Since, this is a SQL-to-SQL ETl, I used the combination of Insert Into and Select statements. We see which queries are ran without any error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21acb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tINSERT INTO songplay (start_time, user_id,level,song_id,artist_id,session_id,location,user_agent)\n",
      "\tSELECT \n",
      "\t\te.ts,\n",
      "\t    e.userid,\n",
      "\t    e.level,\n",
      "\t    s.song_id,\n",
      "\t    s.artist_id,\n",
      "\t    e.sessionid,\n",
      "\t    e.location,\n",
      "\t    e.useragent\n",
      "\tFROM staging_events e\n",
      "\tinner JOIN staging_songs s\n",
      "\tON e.song = s.title\n",
      "\n",
      "\n",
      "\tINSERT INTO users (user_id,first_name,last_name,gender,level)\n",
      "\tSELECT distinct \n",
      "\t\tuserId,\n",
      "\t\tfirstname,\n",
      "\t\tlastname,\n",
      "\t\tgender,\n",
      "\t\tlevel\n",
      "\tFROM staging_events\n",
      "\n",
      "\n",
      "\tINSERT INTO songs (song_id,title,artist_id,year,duration)\n",
      "\tSELECT distinct \n",
      "\t\tsong_id,\n",
      "\t\ttitle,\n",
      "\t\tartist_id,\n",
      "\t\tyear,\n",
      "\t\tduration\n",
      "\tFROM staging_songs\n",
      "\n",
      "\n",
      "\tINSERT INTO artists (artist_id,name,location,latitude,longitude)\n",
      "\tSELECT distinct \n",
      "\t\tartist_id,\n",
      "\t\tartist_name,\n",
      "\t\tartist_location,\n",
      "\t\tartist_latitude,\n",
      "\t\tartist_longitude\n",
      "\tFROM staging_songs\n",
      "\n",
      "\n",
      "\tINSERT INTO playtimes (start_time,hour,day,week,month,year,weekday)\n",
      "\tSELECT start_time,\n",
      "\t\tEXTRACT(HOUR from start_date) as hour,\n",
      "\t\tEXTRACT(DAY from start_date) as day,\n",
      "\t\tEXTRACT(WEEK from start_date) as week,\n",
      "\t\tEXTRACT(MONTH from start_date) as month,\n",
      "\t\tEXTRACT(YEAR from start_date) as year,\n",
      "\t\tEXTRACT(WEEKDAY from start_date) as weekday\n",
      "\tFROM (SELECT ts as start_time, timestamp 'epoch' + cast(ts AS bigint)/1000 * interval '1 second' AS start_date from staging_events)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "etl.insert_tables(cur,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db983cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
